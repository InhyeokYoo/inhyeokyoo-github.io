Text ranking
특정 작업에 대한 쿼리로부터, 코퍼스에서 검색하여 얻어진 ordered list를 생성하는 작업.
폭 넓게는 단순히 키워드로부터 리스트를 얻는 것을 넘어 유저가 연관된 텍스트 정보에 접속할 수 있게 해주는 것을 text ranking이라 함. 이러한 측면에서 보면 QA 또한 text ranking임


Ad hoc retrieval
연관있는 텍스트는 유저가 요청한 토픽과 연관이 있고, 유저의 관심사를 해결해줌


Question Answering
다양한 형태의 QA가 있지만, 대부분의 유저가 현재 경험할 수 있는 것은 SE에서 infobox의 형태로 나타남. 이러한 시스템의 목표는 유저의 질의에 즉각적으로 응답할 수 있는 span을 추출하는 것.

이러한 extractive approach는 두 스텝으로 나누어짐: 정답이 있을만한 passage를 고르고(ranking 포함), answer extraction을 고름.

비록 extractive approach나 factoid question보다는 더 많은 것을 포함하고 있지만, 대부분의 경우엔 여전히 검색하는 것에 의존함


Community Question Answering (CQA)

직접적으로 답을 주는 것보다, 비슷한 질문을 하는 유저의 위치를 표현 (e.g. FAQ, Quora).

이러한 질문에 답변하는 주로 유저가 필요로하는 정보를 다루게 된다.

Exact Match:
Q와 D에 term이 서로 정확히 매칭되어 연관도에 영향을 줌.
BM25는 vector space model에서 term weight을 사용.
Term weighting scheme이 텍스트의 통계적 성질에 따라 term importance를 모델링 할 수 있지만, exact match는 근본적으로 Q와 D가 맞지 않으면 소용이 없음: vocabulary mismatch problem
이에 대한 접근법은 크게 3가지:
1. Enriching query representation: query expansion technique을 사용. 이는 다시 pre/post-retrieval로 나뉨.
2. Enriching Doc. Representation
3. Beyond exact term matching: LDA, LSA처럼 semantic space에서 매칭을 시도

Learning to rank:
SL 사용하여 text ranking을 진행.
pointwise, pairwise, listwise 세 가지 접근법으로 나눔
L2R은 supervised machine-learning approaches to ranking가 동의어가 아님. IR 역사에서 특정 기간동안 등장한 기술을 의미. 예를 들어 transformer같은 경우 L2R이 아닌 SL을 사용하여 랭킹하는 것. L2R은 어떤 모델이냐가 중요한게 아니라, 많은, sparse하고 hand-crafted feature를 썼는가로 구성할 수 있음.


