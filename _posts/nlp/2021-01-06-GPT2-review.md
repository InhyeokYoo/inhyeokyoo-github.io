---
title:  "[작성 중] GPT2: Language Models are Unsupervised Multitask Learners review"
excerpt: "맥락과 함께 자세히 살펴보는 GPT2 논문 리뷰/설명"
toc: true
toc_sticky: true
permalink: /project/nlp/gpt2-review/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling
  - TODO
use_math: true
last_modified_at: 2021-01-06
---

## Introduction

GPT2는 대용량 데이터 셋과 엄청나게 큰 모델을 통해 학습한 언어 모델(language model)로, 다양한 태스크에서 SOTA를 달성하며 zero-shot task trasnfer를 성공적으로 수행해냈다. GPT2 논문을 읽어보며 어떻게 이러한 태스크를 성공적으로 수행했는지 살펴보자.

영문 표현도 복잡하고, 연구 맥락을 몰라 이해하기가 좀 어려웠지만, 최대한 맥락을 이해하려 노력했다. 직독직해하기 어려운 부분이 많아 의역이 많고, 단순한 논문 번역이 아니라 공부한 내용과 사견을 채워넣었기 때문에 본래 논문과 비교해보며 읽으면 좋을 것 같다. 

본 논문은 [다음](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)에서 확인할 수 있다.

## 1. Introduction

기계학습 시스템은 많은 파라미터를 갖는 모델(high-capacity models), 큰 데이터 셋, 그리고 지도 학습을 조합하여 큰 성공을 거뒀다. 그러나 아직은 다루기 어렵고 데이터 분포와 task specification의 약간의 변화에도 민감하다.

현재의 머신러닝은 좁은 범위에 있어서의 전문가이지 (narrow expert), 모든 것을 다루는 제너럴리스트로 보기는 어렵다.
따라서 본 논문은 다양한 테스크를 수행할 수 있는 좀 더 일반적인 시스템에 대해 탐구한다.
이를 통해 결국에는 수동으로 데이터셋을 만들고 레이블링을 할 필요가 없게 될 것이다.

머신러닝의 주 접근법은 원하는 테스크에 대해 맞게 행동하는 학습 셋을 모으고,
이러한 행동을 머신이 따라할 수 있게 하며,
마지막으로 IID 테스트 셋에서 이의 성능을 측정하는 것이다.

이러한 접근법은 narrow expert에서는 성공적이었지만, 다음과 같이 캡셔닝 모델(captioning model)에서의 이상한 행동이라던가 (Lake et al.,2017), 

![image](https://user-images.githubusercontent.com/47516855/103776345-a7677080-5072-11eb-980f-324723f27b11.png){: .align-center}{: width="700"}

기계 독해(reading comprehension)에서의 이상한 행동 이라던가 (Jia & Liang, 2017),

![image](https://user-images.githubusercontent.com/47516855/104025489-10ccb800-5208-11eb-81f9-aabf1aa01773.png){: .align-center}{: width="350"}

이미지 분류(image classification)에서의 이상한 행동 (Alcorn et al., 2018),

![image](https://user-images.githubusercontent.com/47516855/104032675-d405be80-5211-11eb-91ad-0a13534554d0.png){: .align-center}{: width="600"}

들이 발생하는데, 이는 이들과 입력의 다양성에 의해 발생된다.

본 논문은 현재 시스템에서 관찰되는 일반화 성능 부족에 대한 주 원인이 단일 도메인 데이터셋(single domain dataset)에 대한 단일 태스크 학습(single task training) 때문이라고 본다.
현재의 아키텍쳐를 통해 강건한(robust) 시스템을 구축하기 위해서는 넓은 범위의 도메인과 테스크에 대한 학습과 평가가 필요하다.
최근에는 이러한 연구를 시작하기 위해 GLUE, decaNLP와 같은 몇몇 벤치마크가 개발되기도 했다.

Multitask leanring은 일반화 성능 향상시키기에 좋은 방법이지만, 자연어처리에서의 활용은 거의 연구된 바가 없다.
최근 (Yogtama et al., 2019)는 적당한 성능의 향상을 이끌어 내었고, (McCann et al., 2018)과 (Bowman et al., 2018)의 경우 각 10, 17개의 (dataset, objectvie)을 통해 주목할만한 결과를 보였다.

Meta-learning 관점에서 개개의 (dataset, objective)은 dataset과 objective의 분포로부터 샘플링 된 training example로 볼 수 있다.
현재의 머신러닝 시스템에선 이런 일반화 성능을 위해서는 백개에서 천개의 example이 필요하다.
이는 즉, 현재의 기술발전 상황에서 multitask training을 통해 일반화 성능을 향상시키기 위해선 많은 양의 training 쌍이 필요하다는 것을 시사한다.
현재상황에선 이를 브루트 포스를 통해 해결하는데 필요한 만큼 데이터 셋을 생성하거나 목적함수를 설계하는 것을 조절하기 매우 어려운 상황이다.
이로인해 multitask learning을 수행하기 위해 추가적인 환경을 탐색이 필요하다.

현재 language task에 대해 최고의 성능을 내는 시스템은 pre-training과 supervised fine-tuning을 조합하여 활용하는 것이다.
이러한 접근법은 긴 역사를 갖고 있는데, transfer의 더욱 유연한 형태로 나아가는 추세이다.

가장 먼저 (Mikolov et al., 2013) (*오른쪽*), (Collobert et al. 2011) (*왼쪽*)과 같이 task-specific한 구조의 input으로 활용되는 word vector가 있었다.

![image](https://user-images.githubusercontent.com/47516855/104325555-1ab62a00-552c-11eb-849f-6b780fc27877.png){: .align-center}{: width="600"}


그 다음에는 (Dai & Le, 2015)(*위*)나  ELMo (Peters et al., 2018) (*아래*)와 같이 RNN을 활용한 contextual representation을 trasnfer learning하는 형태가 있었고,

![image](https://user-images.githubusercontent.com/47516855/104326639-4a196680-552d-11eb-8fa6-a0ab6f7bcd1d.png){: .align-center}{: width="600"}

최근 연구된 GPT-1(Radford et al., 2018), BERT (Devlin et al., 2018)에서는 task-specific한 구조가 더 이상 필요하지 않고, 많은 self-attention block을 transferring 하는 것만으로도 충분함을 보였다.

이러한 연구들은 태스크를 수행하기 위해 여전히 supervised training을 필요로 한다.
앞선 연구들과는 다른 방향에서 살펴보면, 오직 최소한 혹은 아예 supervised data에 대한 접근이 불가능한 경우에 언어 모델이 commonsense reasoning (Schwartz et al., 2017)이나 sentiment analysis (Radford et al., 2017)과 같은 specific task에서 성공적으로 수행할 수 있음을 증명하였다.

본 논문에서는 방금 살펴본 연구의 두 가지 흐름을 연결하고, transfer의 좀 더 일반적인 방법론을 이어가도록 한다.
본 논문에서는 언어 모델이 zero-shot 환경에서, 어떠한 파라미터나 구조의 변화 없이 downstream task를 수행함을 확인하였다.
이를 통해 zero-shot 환경에서 다양한 범위의 태스크를 수행하는 언어 모델의 능력을 확인할 수 있었다.

## 2. Approach

본 접근법의 핵심은 언어 모델이다. 언어 모델은 주로 unsupervised distribution estimation의 구조를 갖는다.
언어는 자연적으로 순서가 있으므로, 이의 joint probability를 단어로 나누어 조건부 확률의 곱으로 표현하는 것이 당연하다.

$$
p(x) = \displaystyle\prod ^n _1 p(s _n \rvert s _1, ..., s _{n-1})
$$

이러한 접근법은 $p(x)$ 뿐만 아니라 어떠한 조건부 형태인 $p(s _{n-k}, ..., s _n \rvert s _1, ..., s _{n-k-1})$의 추정과 tractable sampling을 가능케한다.
최근 몇년간 트랜스포머와 같이 셀프어텐션 구조를 이용하여 이러한 조건부 확률을 계산하는 모델의 표현력에 상당한 개선이 있었다.

하나의 태스크에 대해 학습하는 것은 조건부 확률 $p(output \rvert input)$를 추정하는 형태의 probabilistic framework로 표현할 수 있다.
일반적인 시스템은 심지어 input이 같은 경우라도 다양한 작업을 수행할 수 있어야 하므로, 이러한 input 뿐만 아니라 태스크에도 조건이 있어야 한다.
따라서 앞선 식은 $p(output \rvert input, task)$가 된다.

이는 multitask와 meta-learning 환경에서 다양하게 사용되어왔다. 이렇게 태스크를 조건으로 주는 것은 (Kaiser et al., 2017)에서의 task specific endoer-dercoder와 같이 구조적인 수준에서 구현되거나,

![image](https://user-images.githubusercontent.com/47516855/104333093-332a4280-5534-11eb-89ea-c8bc60445bae.png){: .align-center}{: width="600"}

MAML (Finn et al., 2017)에서의 inner/outer loop optimization framework과 같이 알고리즘 수준에서 구현된다.
그러나 decaNLP에서 예시를 든 것과 같이 언어를 통해 specific task, inputs, output을 symbol sequence의 형태로 표현할 수 있다.
예를 들어, 번역 문제의 경우 *(translate to french, english text, french text)*와 같이 task, input, output 모두 언어의 형태로 표현할 수 있다.
기계독해 역시 *(answer the question, document, question, answer)*로 표현할 수 있다.

decaNLP는 하나의 모델만을(MQAN) 학습시켜 이러한 형태의 데이터 셋에 대해 다양한 작업을 수행할 수 있음을 보였다.

![image](https://user-images.githubusercontent.com/47516855/104334380-9cf71c00-5535-11eb-877a-be34ea7ab7aa.png){: .align-center}{: width="800"}

또한, symbol이 output으로 나오는 explicit supervision없이 언어 모델은 원칙적으로 decaNLP의 태스크로 학습할 수 있다.
Unsupervised objective는 supervised objective와 동일하지만, 오직 시퀀스의 subset에 대해서만 평가가 가능하기 때문에, unsupervised objective의 global minimum 또한 supervised objective의 global minimum이 된다.

이러한 약간의 toy setting에서, (Sutskeveret al., 2015)에서 논의되었던 principled training objective로서의 density estimation에 대한 우려가 사라지게 된다 (side stepped).
대신, 원칙적으로 unsupervised objective를 최적화하는 것이 문제가 된다.

Preliminary experiments를 통해 이러한 toy-ish set up에서 충분히 큰 언어 모델이 multitask learning을 수행할 수 있음을 확인했지만, explicitly supervised approaches보다는 훨씬 느리게 학습한다.

비록 위에서 언급한 well-posed setup으로부터 "현재 사용중인 언어(language in the wild)"의 복잡함까지 큰 발전을 이뤄냈지만, Weston(2016)은 대화의 맥락(context of dialog)을 통해 자연어로부터 직접 배우는 시스템을 개발할 필요가 있다고 주장했다. 또한, teacher output의 forward prediction에 의한 reward signal없이 QA task를 학습함으로서 이러한 POC를 증명했다 [참고](https://m.blog.naver.com/PostView.nhn?blogId=hist0134&logNo=220916684295&proxyReferer=https:%2F%2Fwww.google.com%2F).

dialog가 매력적인 접근법인 것은 맞으나, 매우 제한되어 있다는 점이 문제가 된다. 인터넷에는 대화형 커뮤니케이션없이 수동적으로 사용할 수있는 방대한 양의 정보가 포함되어 있다. 본 저자들은 충분한 용량을 갖는 언어 모델이 자연어 sequence에서 증명된 task를 수행하고 추론할 수 있을 것이라 추측한다. 만일 추측이 맞다면, 사실상 unsupervised multitask learning을 수행하는 셈이다.

이를 증명하기 위해 넓은 범위의 task에 대해 zero-shot setting으로 언어 모델의 수행 능력을 평가한다.

### 2.1 Training Dataset

대부분의 사전연구는 news article이나 (Jozefowicz et al., 2016), Wikipedia (Merity  et al.,  2016), 소설 책 (Kiroset al., 2015)과 같은 single domain text에 대해 언어 모델을 학습시켰다. 본 논문의 접근법은 가능한 다양한 도메인과 context에서 tasks에 대한 natural language demonstrations를 수집하기위해 가능한 크고 다양한 데이터 셋을 구축하는 것이다.

다양하고 거의 무제한에 가깝게 데이터를 모으는 가장 좋은 방법은 Common Crawl과 같은 웹 크롤링이다. 이러한 archive가 현재 언어 모델 dataset보다 수십배나 많은 데이터 셋을 가지고 있긴 하지만, 품질이 좋지 않다는 것이 문제이다. Trinh & Le (2018)는 Common Crawl을 사용하여 commonsense reasoning을 학습했지만, "대부분의 내용을 이해할 수 없는" 학습 데이터가 많다는 점을 발견했다.

본 논문 역시 이와 비슷한 현상을 발견했다. Trinh & Le (2018)의 실험에서 좋은 결과를 낸 것은 test set인 [Winograd Schema Challenge](https://huggingface.co/datasets/viewer/?dataset=snli)와 유사한 문서를 포함할 때였다. 이러한 접근법이 특정한 task에 대한 성능을 높이는데는 실리적이긴 하지만, 미리 수행하는 작업에 대한 가정을 만드는 것을 피하였다. 

![image](https://user-images.githubusercontent.com/47516855/104456459-08032a00-55ec-11eb-92c1-822ad91c6bb6.png){: .align-center}{: width="800"}
Winograd Schema Challenge는 위 그림과 같이 중의적인 문장을 주고, 단어가 바뀔 때 어떤 뜻이 정답인지를 묻는 task이다. x표시는 dataset-specific bias이다.
{: .notice--info}

이 대신 문서의 품질에 중점을 둔 새로운 web scrape을 만들었다. 이는 사람이 직접 선별하고 필터링 한 것으로, 직접 수동으로 필터링하는 것은 expensive하기 때문에 소셜 미디어 플랫폼인 Reddit에서 최소 3 카르마(일종의 평점) 이상 받은 outbound links를 스크랩했다. 
이는 다른 사람이 이 링크를 흥미, 교육, 아니면 단순히 재미있게 생각하는지에 대한, 일종의 휴리스틱한 지표로 생각할 수 있다.

이에 대한 결과인 WebText는 45M 개의 링크에 대한 부분 집합을 포함하고 있다. HTML response로부터 텍스트를 추출하기 위해 Dragnet (Peters &Lecocq, 2013)과 [Newspaper](https://github.com/codelucas/newspaper) content extractor를 조합하여 사용하였다.

본 논문의 결과는 전부 WebText의 이전버전으로 2017년 이후의 링크는 포함하고 있지 않으며, total 40GB 텍스트를, 8M 조금 넘는 문서에 대해 중복 제거와 휴리스틱한 정제 과정을 거쳤다. 데이터 셋 내의 위키피디아 문서는 다른 데이터 셋에서도 발견할 수 있는 일반적인 셋이므로 학습/테스트 평가간의 중복으로 인해 분석하기가 까다로우므로 전부 삭제하였다.

### 2.2. Input Representation

일반적인 언어 모델은 어떠한 문자열 (string)에 대한 확률과 이의 생성이 가능해야 한다. 현재의 커다란 언어 모델은 소문자, 토큰화, out of vocabulary (OOV) 토큰 문제가 있어 **모델이 사용가능한 문자열의 공간을 제한**한다. Gillicket al. (2015)의 예시처럼 **유니코드 문자열을 UTF-8 바이트 시퀀스로 사용**하는 것은 이러한 요구조건을 깔끔하게 해결하지만, One Billion Word Benchmark (Al-Rfou et al., 2018)과 같은 **대규모의 데이터 셋**에서는 현재의 바이트 단위 (byte-level) 언어 모델은 **단어 단위 (word-level) 언어 모델과 경쟁할 급이 안된다.** GPT2도 WebText에 대해 일반적인 바이트 단위 언어 모델을 시도했을 때도 비슷한 성능의 차이를 관측할 수 있었다.

Byte Pair Encoding (BPE) (Sennrich et al., 2015)은 단어 단위 언어 모형과 문자 단위 (character-level) 언어 모형 사이의 실용적인 절충안으로, **자주 등장하는 심볼 시퀀스는 단어 단위로, 자주 등장하지 않는 심볼 시퀀스는 문자 단위로 취급**한다. 
이름이 무색하게도 BPE의 구현은 바이트가 아닌 유니코드 코드 포인트에 대해 동작한다.
따라서 모든 유니코드 문자열을 모델링하기 위해 유니코드 심볼의 full space를 필요로 할 것이다.
이로 미루어볼 때 단어를 추가하기도 전인 기본 사전 (base vocabulary)만으로 130,000개의 사이즈를 갖는다는 결론이 나온다.
이는 일반적으로 BPE에서 사용되는 32,000 ~ 64,000 토큰 사전에 비교했을 때 엄청나게 큰 숫자이다.

반면, 바이트 단위 BPE는 기본 사전으로 256의 사이즈만을 필요로 한다.
그러나 바이트 시퀀스에 대해 직접적으로 BPE를 적용하는 것은 차선책(sub-optimal)으로, 이는 토큰 사전을 구축하기 위해 greedy frequency 기반의 휴리스틱 알고리즘을 사용하기 때문이다. BPE를 적용하면 *dog*와 같은 일반적인 단어의 다양한 변형인 *dog.*, *dog!*, *dog?*등이 함께 사전에 포함되는 것을 확인할 수 있다.  
이러한 문제를 피하기 위해, BPE가 어떠한 바이트 시퀀스에 대해서도 character category를 넘어 합쳐지는 것을 방지한다.
그러나 공백에 대해서는 예외를 두는데, 공백은 압축 효율을 엄청나게 상승시키면서 동시에 여러개의 토큰으로 표현하는 단어를 최소한의 분열만을 통해 사전에 추가할 수 있기 때문이다.

이러한 입력 표현(input representation)은 **바이트 단위 접근법의 일반성과 단어 단위 언어모형의 경험적 이점**을 결합할 수 있도록 해준다.
이를 통해 어떠한 유니코드 문자열에 확률을 부여할 수 있게 되고, 전처리나 토큰화, 사전 크기와 무관하게 어떠한 데이터셋에 대해서도 GPT2를 평가할 수 있게 된다.

여기서 말하는 Unicode string/byte-level의 차이가 뭔지 모르겠다. 어차피 결과가 똑같을텐데 다르다는 말인지...
{: .notice--danger}

### 2.3. Model

모델에서는 딱히 특별한 것은 없다. Transformer를 사용했고, OpenAI GPT의 구조와 거의 동일하다.
다만 약간의 수정이 있는데, 우선 layer normalization이 각 sub-block의 입력으로 오게 됐고, 마지막 self-attention block 다음에 layer normalization을 하나 더 추가했다.
모델의 깊이에 따른 residual path의 누적을 설명하기 위해 수정된 초기화를 사용하였고, residual layer의 가중치는 이의 개수 $N$의 루트 분의 1인 $\frac{1}{\sqrt N}$로 스케일하였다.
사전의 크기는 50,257로 확장되었고, 시퀀스의 크기 또한 512에서 1024 토큰으로 늘렸다. 배치사이즈는 512이다.

## 3. Experiments

대략 log-uniformly한 사이즈를 갖는 4가지의 언어모델에 대해 학습 및 벤치마크를 실시한다. 아래 테이블에 모델 구조가 요약되어 있다.

![image](https://user-images.githubusercontent.com/47516855/107938412-b40faa80-6fc8-11eb-8c1f-5e5d2c20f551.png){: .align-center}{: width='500'}

가장 작은 모델은 GPT-1과 동일한 사이즈를 갖고 있고, 2번째로 작은 모델은 BERT와 같은 사이즈를 갖는다. 가장 큰 모델인 GPT-2는 GPT-1에 비해 10배나 많은 파라미터를 갖고 있다.

각 모델의 learning rate는 WebText의 5%에 해당하는 hold-out (test or validation)에 대해 가장 좋은 perplexity를 갖도록 수동으로 튜닝한다. 모든 모델은 여전히 WebText에 대해 과소적합되어 있고, hold-out perplexity 또한 학습할수록 성능이 좋아진다.


### 3.1 Language Modeling

zero-shot task transfer를 향한 첫 걸음으로서, 언어모델에 대해 학습했을 때 WebText가 zero-shot 도메인 transfer을 어떻게 수행하는지를 이해하는 것부터 시작해보자.

모델은 바이트 단위로 동작하고 토큰화 과정이나 전처리 과정에서 손실이 생기지 않기 때문에 어떠한 벤치마크 데이터셋에 대해서도 평가할 수 있다. 언어 모형 데이터 셋에 대한 결과는 문자(character), 바이트, 단어 당 음의 로그 확률값의 평균 (average negative log probability)을 스케일하거나 지수적으로 표현하는게 일반적이다. 따라서 GPT-2도 이와 동일하게 표현하여 성능을 비교한다.

이러한 데이터셋에 대해 WebText 언어모델은 학습된 데이터와 **상당히 다른 분포를 갖는 상태**에서 테스트된다 (out-of-distribution). 이는 매우 정규화된 텍스트와 토큰 (분리된 구둣점이나 문장 셔플링, 축약어(e.g. we're)), WebText에서는 매우 희귀한(400 바이트 중 오직 26번만 등장) \<UNK>를 예측하도록 한다. 이에 대한 결과는 Table 3에 나타나있다. 이는 역토크나이저(de-tokenizer)를 사용한 결과이다. 이런 역토크나이저는 인버터블하므로, 여전히 로그 확률값을 계산할 수 있고, 이러한 역토크나이저는 도메인 적응(domain adaptaion)의 간단한 표현으로도 볼 수 있다. 이러한 역토크나이저의 결과로 2.5 perplexity에서 5로 증가하는 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/47516855/107952116-af54f180-6fdc-11eb-89b2-fa623215f7b7.png){: .align-center}{: width='800'}

WebText 언어모델은 도메인과 데이터셋 사이를 잘 전이(transfer) 할 수 있고, 제로샷상태에서 8개 중 7 데이터 셋에 대해 SOTA를 달성했다. Penn Treebank와 WikiText-2와 같이 오직 100M - 200M개의 토큰이 필요한 작은 데이터 셋에서 가장 큰 향상이 일어났다는 것이 눈여겨볼만하다.

또한, LAMBADA나 Childre's Book Test와 같이 long-term dependency를 측정하기 위한 데이터셋에서 큰 발전이 있었던 것도 주목할만하다.

본 모델은 One Billion Word Benchmark에 대해서는 매우 안 좋은 결과를 보였는데, 이는 1BW가 매우 큰 데이터 셋이고, 문장 단위의 셔플링으로 인해 긴 문장의 구조를 끊음으로 생기는 현상으로 보인다.

### 3.2 Childre's Book Test

![](https://paperswithcode.com/media/datasets/Screenshot_2021-01-27_at_13.46.56.png){: .align-center}{: width='700'}

[Childre's Book Test 설명 (papers with code)](https://paperswithcode.com/dataset/cbt)

CBT는 서로 다른 카테고리(name entity, noun, verb, preposition)에 대해 언어모델의 성능을 평가한다. 따라서 perplexity로 성능을 평가하는 것보단 정확도를 통해 성능을 평가한다. CBT는 자동으로 생성된 cloze test에 대해 10개의 후보를 주고 정답을 맞추는 식으로 진행된다.

Figure 2에서 볼 수 있듯, 모델사이즈가 증가할수록 점차 성능이 증가하며 인간의 성능에 가까워 지는 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/47516855/107953954-2c816600-6fdf-11eb-8c2a-141970c32cf9.png){: .align-center}{: width='500'}

CBT의 테스트셋에 있는 정글북이 WebText에 있는 것을 확인하였고, 따라서 이는 검증셋에서 제외하고 성능을 평가하였다. GPT-2는 일반명사에 대해 93.3%, named entity에 대해 89.1%로 새로운 SOTA를 달성하였다. 역토크나이저는 PTB 스타일의 토큰화를 제거하기 위해 적용되었다.

### 3.3 LAMBADA

LAMBADA는 (Paperno et al., 2016) 텍스트 내 긴 문장에 대한 의존성을 테스트하는 데이터셋으로, 문장의 마지막 단어를 예측하는 것을 목표로 한다. 

![](https://paperswithcode.com/media/datasets/LAMBADA-0000002422-52650e4e_B4dJstl.jpg){: .align-center}{: width='700'}

[LAMBADA 설명 (papers with code)](https://paperswithcode.com/dataset/lambada)

각 문장은 최소 50개 이상의 토큰으로 이루어져 있다. GPT-2는 이전 SOTA인 99.8을 갈아치운 8.6의 perplexity를 달성하였고 (Grave et al., 2016), 정확도 또한 19% (Dehghani et al., 2018)보다 높은 52.66%를 달성하였다.

GPT-2가 오류내는 것을 살펴보면 **마지막 단어로는 부적절하지만, 유효한 문장**임을 확인할 수 있다. 이로 미루어 보았을 때, 언어모델은 문장의 마지막 단어라는 유용한 정보를 사용하지 않는 것으로 보인다. 

불용어(stop-word) 필터를 사용하였을 때 대략 정확도가 63.24%가 오른 것을 볼 수가 있고, 이에 대한 전반적인 SOTA를 4% 올릴 수 있었다. 이전 SOTA는 (Hoang et al., 2018) 마지막 단어를 예측하는데 제약을 두었는데, context에 등장한 단어만을 예측하도록 하였다. GPT-2에 경우엔 이러한 제약은 오히려 단점이 되는데, 정답의 19%가 context에 등장하지 않기 때문이다. 아무래도 이는 GPT-2가 원체 성능이 좋으므로 딱히 시도하지 않았다는 것으로 보인다. 데이터셋은 전처리를 하지 않은 버전으로 진행하였다.

### 3.4 Winograd Schema Challenge

Winograd Schema challenge ([Levesque et al., 2012](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492))는 상식추론(commonsense reasoning) 능력을 측정하는 데이터셋으로, 텍스트 내 모호함을 해결함으로써 이를 측정한다.

![](https://paperswithcode.com/media/datasets/WSC-0000002428-aeeee073_CkYkrdR.jpg){: .align-center}{: width='500'}

[LAMBADA 설명 (papers with code)](https://paperswithcode.com/dataset/wsc)

 최근에 [Trinh & Le (2018)](https://arxiv.org/abs/1806.02847)는 **언어모델이 높은 확률을 할당하는 단어**를 통해 resolution을 해결하였다. GPT-2는 이러한 접근법을 따라서 진행했다. 아래의 Figure 3에 이에 대해 자세하게 나와 있다. Partial Scoring과 Full scoring의 차이는 일부만 맞추는 경우와 전체 다 맞추는 경우를 구분한 것으로 보인다. 원래는 A와 B 보기 중 맞추는 문제인데, 이를 LM으로 해결하다 보니 부분적으로만 맞는 것도 있기 때문이다.

![image](https://user-images.githubusercontent.com/47516855/107967817-f3ea8800-6ff0-11eb-87dd-83f7576b06c1.png){: .align-center}{: width='500'}

GPT-2는 SOTA 정확도에서 7% 높은 70.7%를 달성하였다. 데이터셋은 273개의 예시로, 꽤나 작기 때문에 [Trichelair et al. (2018)](https://arxiv.org/abs/1811.01778)를 읽고 이러한 결과에 대한 맥락을 이해하길 추천한다. [ACL 발표영상](https://vimeo.com/426363035)


### 3.5 Reading Comprehension

기계독해(reading comprehension)는 질문이 문서나 문단에 관한 것이고, 답변은 문서 내에 있어서 이를 찾는 것을 의미한다. Conversation  Question  Answering  dataset (CoQA) ([Reddy et al. (2018)](https://arxiv.org/pdf/1808.07042v2.pdf))은 7개 다른 도메인의 문서로 이루어져있고, 질문자와 답변자가 문서에 대해 대화하는 형식으로 되어 있다.

![image](https://user-images.githubusercontent.com/47516855/105998936-7538b400-60f0-11eb-836d-13c7ef86e2ef.png){: .align-center}{: width='400'}

[CoQA 설명 (papers with code)](https://paperswithcode.com/dataset/coqa)

CoQA는 기계독해 능력과, 대화 기록에("Why?"와 같이) 따른 질문에 답변하는 능력을 테스트한다.

문서와 이와 관련된 대화 기록(위 사진의 Q1, Q2 등), 그리고 **마지막 토큰 `A:`**를 조건부로 GPT-2로부터 그리디 디코딩하는 경우 55의 F1 score를 달성하였다.

여기서 `A:`을 조건부로 주는 것은 웹 크롤링을 통해 모은 WebText만의 특성이라고 볼 수 있다. GPT-2는 언어모델이기 때문에 질의응답 테스크를 따로 학습하지 않았다. 이런 상황에서 질문에 따른 응답을 얻으려면 어떻게 해야할까? 정답은 아래와 같이 우리가 인터넷에서 흔히 볼 수 있는 Q:, A:를 이용하는 것이다. ![image](https://user-images.githubusercontent.com/47516855/107970714-d0294100-6ff4-11eb-809e-70722a8f6f6e.png){: .align-center}{: width='600'}
{: .notice--info}

이는 127,000 이상의 수동으로 수집한 질문, 정답 쌍에 대해 학습한 baseline 4개 중 3개와 일치하거나 능가하는 성능을 보인다. 지도학습한 SOTA인 BERT의 경우 89의 F1 score를 달성, 인간의 성능과 거의 비슷하다. 반면 GPT-2는 어떠한 지도학습을 하지 않은 시스템 중 가장 좋은 성능을 냈고, 정답과 에러에 대한 분석 결과 GPT-2는 종종 *'누구'가 들어간 질문에 대한 답변으로 문서 내 이름을 답변하는 식*의  간단한 검색 기반의 휴리스틱한 알고리즘을 사용하는 것으로 보인다.

### 3.6 Summarization

### 3.7 Translation

### 3.8 Question Answering

> The simplest type of question answering systems are dealing with factoid questions(Jurafsky and Martin, 2008). The answer of this type of questions are **simply one or more words which gives the precise answer of the question**. For example questions like “What is a female rabbit called?” or “Who discovered electricity?” are factoid questions. Sometimes the question asks for a body of information instead of a fact. For example questions like “What is gymnophobia?” or “Why did the world enter a global depression in 1929?” are of these type.To answer these questions typically a summary of one or more documents should be given to the user. (Loni, 2014)

> A non-factoid question answering (QA) is an umbrella term that covers all question-answering topics beyond factoid question answering. As a quick reminder: a factoid QA is about **providing concise facts**. For example, "who is the headmaster of Hogwarts?", "What is the population of Mars", and so on, so forth. [Quora: Natural Language Processing: What is "Non-factoid question answering"?](https://www.quora.com/Natural-Language-Processing-What-is-Non-factoid-question-answering)

> We find it encouraging that the model can remember facts, understand  contexts, perform  common  sense  reasoning without the complexity in traditional pipelines.  What surprises  us  is  that  the  model  does  so  without  any  explicit knowledge representation component except for the parameters in the word vectors. Perhaps  most  practically  significant  is  the  fact  that  the model can  generalize to  new  questions.   In  other  words, it does not simply look up for an answer by matching the question with  the existing database.   In fact,  most of the questions presented above, except for the first conversation,do not appear in the training (Vinyals & Le, 2015).