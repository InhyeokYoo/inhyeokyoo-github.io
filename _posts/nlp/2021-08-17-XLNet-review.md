---
title:  "XLNet: Generalized Autoregressive Pretraining for Language Understanding review"
toc: true
toc_sticky: true
permalink: /project/nlp/XLNet-review/
categories:
  - NLP
  - Paper Review
tags:
  - Language Modeling
use_math: true
last_modified_at: 2021-08-17
---

## 들어가며

## Abstract

양방향의 맥락을 모델링하는 능력을 통해 BERT와 같은 denoising autoencoding 기반의 pretraining 모델은 autoregressive language modeling 기반의 pretraining 방법보다 더 나은 성능을 보였다. 

## 1.



{: .align-center}{: width="700"}

{: .text-center}
